library("rJava")
library("RWekajars")
library("RWeka")

WPM("refresh-cache")
WPM("package-info", "repository", "naiveBayesTree")
WPM("install-package", "naiveBayesTree")

#WOW("weka/classifiers/trees/NBTree")

NBTree <- make_Weka_classifier("weka/classifiers/trees/NBTree")

sink("~/Desktop/output.txt", append=FALSE, split=FALSE)


naive_bayes1 <- function(trainset, testset){
  library(caret)
  #Naive Bayes Classifier
  # Installing e1071
  library(e1071)
  # Applying Naive Bayes
  nbc <- naiveBayes(class ~., data =trainset )
  # Making predictions
  prediction2 <- predict(nbc,  testset, type="class")
  #cm2 <- table(testset$class, prediction2, dnn = c("Real Values", "Predictions"))
  #Calculating Accuracy
  #n2 = diag(cm2)
  #accuracy2 <- sum(n2)/sum(cm2)
  #accuracy_list2[j] = accuracy2
  count<- 0
  for(j in 1:length(prediction2)){
    if(prediction2[j]==testset$class[j]){
      count = count + 1
    }
  }
  res <- count/length(prediction2)
  return(res)
}

c4.51 <- function(trainset, testset){
  library(caret)
  library(RWeka)
  set.seed(10)
  C4.5 <- J48(class ~., data = trainset)
  # Making predictions
  prediction2 <- predict(C4.5, testset, type="class")
  cm2 <- table(testset$class, prediction2, dnn = c("Real Values", "Predictions"))
  #Calculating Accuracy
  n2 = diag(cm2)
  accuracy2 <- sum(n2)/sum(cm2)
  # count<- 0
  # for(j in 1:length(prediction2)){
  #   if(prediction2[j]==testset$class[j]){
  #     count = count + 1
  #   }
  # }
  # res <- count/length(prediction2)
  return(accuracy2)
}

c5.01 <- function(trainset, testset){
  library(caret)
  set.seed(10)
  library(C50)
  C5 <- C5.0(class~., data=trainset, trials = 5, control=C5.0Control(winnow=TRUE, CF=0.25))
  plot(C5)
  print(summary(C5))
  # Making predictions
  prediction2 <- predict(C5, testset, type="class")
  count<- 0
  for(j in 1:length(prediction2)){
    if(prediction2[j]==testset$class[j]){
      count = count + 1
    }
  }
  res <- count/length(prediction2)
  return(res)
}

nbtree1 <- function(trainset, testset){
  library(caret)
  

  
  nbTree <- NBTree(class ~ ., data=trainset)
  # Making predictions
  prediction2 <- predict(nbTree, testset, type="class")
  #cm2 <- table(testset$class, prediction2, dnn = c("Real Values", "Predictions"))
  #Calculating Accuracy
  #n2 = diag(cm2)
  #accuracy2 <- sum(n2)/sum(cm2)
  #accuracy_list2[j] = accuracy2
  count<- 0
  for(j in 1:length(prediction2)){
    if(prediction2[j]==testset$class[j]){
      count = count + 1
    }
  }
  res <- count/length(prediction2)
  return(res)
}

do_method1 <- function(train, test){
  res = c()
  nb = naive_bayes1(train, test)
  c4.5= c4.51(train, test)
  c5 = c5.01(train, test)
  nbtree = nbtree1(train, test)
  print(nb)
  print(c4.5)
  print(nbtree)
  res[1] = (nbtree - nb)*100
  res[2] = (nbtree - c4.5)*100
  res[3] = (1-nbtree)/(1-nb)
  res[4] = (1-nbtree)/(1-c4.5)
  res[5] = (nbtree - c5)*100
  return(res)
  
}


########################################
naive_bayes <- function(data, k){
  library(caret)

  set.seed(10)
  #Data Partitioning
  trainIndices <- createDataPartition(y = data$class, p=k, list = FALSE)
  #trainIndices = sample(1:nrow(ecoli),nrow(ecoli)*0.8)
  trainset <- data[trainIndices, ]
  testset <- data[-trainIndices, ]
  
  #Naive Bayes Classifier
  # Installing e1071
  library(e1071)
  # Applying Naive Bayes
  nbc <- naiveBayes(class ~., data =trainset )
  # Making predictions
  prediction2 <- predict(nbc, testset, type="class")
  #cm2 <- table(testset$class, prediction2, dnn = c("Real Values", "Predictions"))
  #Calculating Accuracy
  #n2 = diag(cm2)
  #accuracy2 <- sum(n2)/sum(cm2)
  #accuracy_list2[j] = accuracy2
  count<- 0
  for(j in 1:length(prediction2)){
    if(prediction2[j]==testset$class[j]){
      count = count + 1
    }
  }
  res <- count/length(prediction2)
  return(res)
}

c4.5 <- function(data, k){
  library(caret)
  
  set.seed(10)
  #Data Partitioning
  trainIndices <- createDataPartition(y = data$class, p=k, list = FALSE)
  #trainIndices = sample(1:nrow(ecoli),nrow(ecoli)*0.8)
  trainset <- data[trainIndices, ]
  testset <- data[-trainIndices, ]
  
  C4.5 <- J48(class ~., data = trainset)
  # Making predictions
  prediction2 <- predict(C4.5, testset, type="class")
  #cm2 <- table(testset$class, prediction2, dnn = c("Real Values", "Predictions"))
  #Calculating Accuracy
  #n2 = diag(cm2)
  #accuracy2 <- sum(n2)/sum(cm2)
  #accuracy_list2[j] = accuracy2
  count<- 0
  for(j in 1:length(prediction2)){
    if(prediction2[j]==testset$class[j]){
      count = count + 1
    }
  }
  res <- count/length(prediction2)
  return(res)
}

c5.0 <- function(data, k){
  library(caret)
  set.seed(10)
  #Data Partitioning
  trainIndices <- createDataPartition(y = data$class, p=k, list = FALSE)
  #trainIndices = sample(1:nrow(ecoli),nrow(ecoli)*0.8)
  trainset <- data[trainIndices, ]
  testset <- data[-trainIndices, ]
  library(C50)
  C5 <- C5.0(class~., data=trainset, trials = 5, control=C5.0Control(winnow=TRUE, CF=0.25))
  # Making predictions
  prediction2 <- predict(C5, testset, type="class")
  count<- 0
  for(j in 1:length(prediction2)){
    if(prediction2[j]==testset$class[j]){
      count = count + 1
    }
  }
  res <- count/length(prediction2)
  return(res)
}

nbtree <- function(data, k){
  library(caret)
  
  set.seed(10)
  #Data Partitioning
  trainIndices <- createDataPartition(y = data$class, p=k, list = FALSE)
  #trainIndices = sample(1:nrow(ecoli),nrow(ecoli)*0.8)
  trainset <- data[trainIndices, ]
  testset <- data[-trainIndices, ]
  
  nbTree <- NBTree(class ~ ., data=trainset)
  # Making predictions
  prediction2 <- predict(nbTree, testset, type="class")
  #cm2 <- table(testset$class, prediction2, dnn = c("Real Values", "Predictions"))
  #Calculating Accuracy
  #n2 = diag(cm2)
  #accuracy2 <- sum(n2)/sum(cm2)
  #accuracy_list2[j] = accuracy2
  count<- 0
  for(j in 1:length(prediction2)){
    if(prediction2[j]==testset$class[j]){
      count = count + 1
    }
  }
  res <- count/length(prediction2)
  return(res)
}

do_method <- function(data, k){
  res = c()
  nb = naive_bayes(data, k)
  c4.5= c4.5(data, k)
  nbtree = nbtree(data, k)
  c5 = c5.0(data, k)
  print(nb)
  print(c4.5)
  print(nbtree)
  res[1] = (nbtree - nb)*100
  res[2] = (nbtree - c4.5)*100
  res[3] = (1-nbtree)/(1-nb)
  res[4] = (1-nbtree)/(1-c4.5)
  res[5] = (nbtree - c5) * 100
  return(res)
  
}

cross_validation_naive_bayes <- function(yourData, k){
  require(caret)
  set.seed(100)
  folds<-createFolds(y=yourData$class,k)
  accuracy_list2=c()
  #Perform k fold cross validation
  for(i in 1:k){
    #Segement your data by fold using the which() function 
    train_cv<-yourData[-folds[[i]],]
    test_cv<-yourData[folds[[i]],]
    #Naive Bayes Classifier
    # Installing e1071
    library(e1071)
    # Applying Naive Bayes
    nbc <- naiveBayes(class ~., data =train_cv )
    # Making predictions
    prediction2 <- predict(nbc, test_cv, type="class")
    # count<- 0
    # for(j in 1:length(prediction2)){
    #   if(prediction2[j]==test_cv$class[j]){
    #     count = count + 1
    #   }
    # }
    # res <- count/length(prediction2)
    # accuracy_list2[i] = res
    cm1 <- table(test_cv$class, prediction2, dnn = c("Real Values", "Predictions"))
    #Calculating Accuracy
    n1 = diag(cm1)
    accuracy1 <- sum(n1)/sum(cm1)
    accuracy_list2[i] = accuracy1
  }
  return(accuracy_list2)
}


cross_validation_nbtree <- function(yourData, k){
  require(caret)
  set.seed(100)
  folds<-createFolds(y=yourData$class,k)
  accuracy_list2=c()
  #Perform k fold cross validation
  for(i in 1:k){
    #Segement your data by fold using the which() function 
    train_cv<-yourData[-folds[[i]],]
    test_cv<-yourData[folds[[i]],]
    #nbtree
    nbTree <- NBTree(class ~ ., data=train_cv)
    # Applying Naive Bayes
  
    # Making predictions
    prediction2 <- predict(nbTree, test_cv, type="class")
    count<- 0
    for(j in 1:length(prediction2)){
      if(prediction2[j]==test_cv$class[j]){
        count = count + 1
      }
    }
    res <- count/length(prediction2)
    accuracy_list2[i] = res
  }
  return(accuracy_list2)
}

cross_validation_c4.5 <- function(yourData, k){
  require(caret)
  set.seed(100)
  folds<-createFolds(y=yourData$class,k)
  accuracy_list2=c()
  #Perform k fold cross validation
  for(i in 1:k){
    #Segement your data by fold using the which() function 
    train_cv<-yourData[-folds[[i]],]
    test_cv<-yourData[folds[[i]],]
    #nbtree
    C4.5 <- J48(class ~., data = train_cv)
    
    # Making predictions
    prediction2 <- predict(C4.5, test_cv, type="class")
    # count<- 0
    # for(j in 1:length(prediction2)){
    #   if(prediction2[j]==test_cv$class[j]){
    #     count = count + 1
    #   }
    # }
    # res <- count/length(prediction2)
    # accuracy_list2[i] = res
    cm1 <- table(test_cv$class, prediction2, dnn = c("Real Values", "Predictions"))
    #Calculating Accuracy
    n1 = diag(cm1)
    accuracy1 <- sum(n1)/sum(cm1)
    accuracy_list2[i] = accuracy1
  }
  return(accuracy_list2)
}

cross_validation_c5.0 <- function(yourData, k){
  require(caret)
  set.seed(100)
  folds<-createFolds(y=yourData$class,k)
  accuracy_list2=c()
  #Perform k fold cross validation
  for(i in 1:k){
    #Segement your data by fold using the which() function 
    train_cv<-yourData[-folds[[i]],]
    test_cv<-yourData[folds[[i]],]
    #nbtree
    library(C50)
    C5 <- C5.0(class~., data=train_cv, trials = 5, control=C5.0Control(winnow=TRUE, CF=0.25))
    
    # Making predictions
    prediction2 <- predict(C5, test_cv, type="class")
    # count<- 0
    # for(j in 1:length(prediction2)){
    #   if(prediction2[j]==test_cv$class[j]){
    #     count = count + 1
    #   }
    # }
    # res <- count/length(prediction2)
    # accuracy_list2[i] = res
    cm1 <- table(test_cv$class, prediction2, dnn = c("Real Values", "Predictions"))
    #Calculating Accuracy
    n1 = diag(cm1)
    accuracy1 <- sum(n1)/sum(cm1)
    accuracy_list2[i] = accuracy1
  }
  return(accuracy_list2)
}

do_cross_validation <- function(data, k){
  res = c()

  nb_cv = cross_validation_naive_bayes(data, k)
  v1 = sum(nb_cv)/k
  
  c4.5_cv = cross_validation_c4.5(data, k)
  v2 =sum(c4.5_cv)/k
  
  nbtree_cv = cross_validation_nbtree(data, k)
  v3 = sum(nbtree_cv)/k
  
  c5_cv = cross_validation_c5.0(data, k)
  v4 = sum(c5_cv)/k
  print(v1)
  print(v2)
  print(v3)
  
  res[1] = (v3-v1)*100
  res[2] = (v3-v2)*100
  res[3] = (1-v3)/(1-v1)
  res[4] = (1-v3)/(1-v2)
  res[5] = (v3-v4)*100
  return(res)
  
}

nb_nbtree_diff = c()
c4.5_nbtree_diff = c()
c5_nbtree_diff = c()
nb_nbtree_error = c()
c4.5_nbtree_error = c()

index = 1
names = c()

# tictactoe dataset
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data"
tic_tac_toe <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ","))
colnames(tic_tac_toe)[10] <- "class"
res = do_cross_validation(tic_tac_toe, 10)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Tictactoe"
index = index + 1


# chess
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/chess/king-rook-vs-king-pawn/kr-vs-kp.data"
krkp <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ","))
colnames(krkp)[37] <- "class"
res = do_method(krkp, 1/3)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Chess"
index = index + 1


#letter
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data"
letter <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ","))
colnames(letter)[1] <- "class"
res = do_method(letter, 1/3)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Letter"
index = index + 1

# vote dataset
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data"
vote <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ","))
#colnames(ecoli) <- c("Sequence Name", "mcg", "gvh", "lip", "chg", "aac", "alm1", "alm2","class")
vote <- na.omit(vote)
colnames(vote)[1] <- "class"
res = do_cross_validation(vote, 10)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Vote"
index = index + 1


# monk1
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.train"
monk1_train <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ""))
monk1_train <- monk1_train[-8]
colnames(monk1_train)[1] <- "class"
monk1_train[1] <- as.factor(x = monk1_train [[1]])
library(plyr)
monk1_train$class <- revalue(monk1_train$class, c("1" = "class1",
                                                  "0" = "class0"))
for (i in 1:7){
  monk1_train[i] <- as.factor(x = monk1_train[[i]])
}


url= "https://archive.ics.uci.edu/ml/machine-learning-databases/monks-problems/monks-1.test"
monk1_test <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ""))
monk1_test <- monk1_test[-8]
colnames(monk1_test)[1] <- "class"
monk1_test[1] <- as.factor(x = monk1_test [[1]])
library(plyr)
monk1_test$class <- revalue(monk1_test$class, c("1" = "class1",
                                                "0" = "class0"))
for (i in 1:7){
  monk1_test[i] <- as.factor(x = monk1_test[[i]])
}


res = do_method1(monk1_train, monk1_test)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Monk1"
index = index + 1

# iris
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
iris <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ","))
colnames(iris)[5] <- "class"
for (i in 1:4){
  iris[i] <- as.factor(x = iris[[i]])
}

res = do_cross_validation(iris, 10)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Iris"
index = index + 1


#  mushroom
# url= "https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data"
# mushroom <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ",", na.strings="?"))
# #colnames(ecoli) <- c("Sequence Name", "mcg", "gvh", "lip", "chg", "aac", "alm1", "alm2","class")
# colnames(mushroom)[1] <- "class"
# mushroom=na.omit(mushroom)
# res = do_method(mushroom, 1/3)
# nb_nbtree_diff[index] = res[1]
# c4.5_nbtree_diff[index] = res[2]
# nb_nbtree_error[index] = res[3]
# c4.5_nbtree_error[index] = res[4]
# names[index] = "Mushroom"
# index = index + 1
# 
# soybean dataset
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-large.data"
soybean_data <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = "," , na.strings = "?"))
soybean_data <- na.omit(soybean_data)
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-large.test"
soybean_test <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = "," , na.strings = "?"))
soybean_test <- na.omit(soybean_test)
soybean <- rbind(soybean_data, soybean_test)
#colnames(ecoli) <- c("Sequence Name", "mcg", "gvh", "lip", "chg", "aac", "alm1", "alm2","class")
colnames(soybean)[1] <- "class"
for (i in 2:36){
  soybean[i] <- as.factor(x = soybean[[i]])
}
res = do_cross_validation(soybean, 10)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Soybean"
index = index + 1
# 
# cancer(L)
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data"
cancer <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ",", na.strings = "?"))
cancer <- na.omit(cancer)
#colnames(ecoli) <- c("Sequence Name", "mcg", "gvh", "lip", "chg", "aac", "alm1", "alm2","class")
colnames(cancer)[1] <-"class"
cancer[7] <- as.factor(x = cancer[[7]])
res = do_cross_validation(cancer, 10)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Cancer(L)"
index = index + 1
# 
# cancer(W) dataset
url= "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
cancer <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ",", na.strings = "?"))
cancer <- na.omit(cancer)
#colnames(ecoli) <- c("Sequence Name", "mcg", "gvh", "lip", "chg", "aac", "alm1", "alm2","class")
colnames(cancer)[11] <- "class"
cancer[11] <- as.factor(x = cancer [[11]])
library(plyr)
cancer$class <- revalue(cancer$class, c("2" = "benign", "4" = "malignant" ))
cancer = cancer[-1]
res = do_cross_validation(cancer, 10)
nb_nbtree_diff[index] = res[1]
c4.5_nbtree_diff[index] = res[2]
nb_nbtree_error[index] = res[3]
c4.5_nbtree_error[index] = res[4]
c5_nbtree_diff[index] = res[5]
names[index] = "Cancer(W)"
index = index + 1

# # glass dataset
# #Getting Dataset chess
# url= "https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data"
# glass <-  as.data.frame(read.table(file = url, header = FALSE, dec = ".", sep = ","))
# colnames(glass)[11] <- "class"
# glass[11] <- as.factor(x = glass [[11]])
# library(plyr)
# glass$class <- revalue(glass$class, c("1" = "building_windows_float_processed",
#                                       "2" = "building_windows_non_float_processed",
#                                       "3" = "vehicle_windows_float_processed",
#                                       "5" = "containers",
#                                       "6" = "tableware",
#                                       "7" = "headlamps"))
# 
# glass=glass[-1]
# for (i in 1:9){
#   glass[i] <- as.factor(x = glass[[i]])
# }
# res = do_cross_validation(glass, 10)
# res
# nb_nbtree_diff[index] = res[1]
# c4.5_nbtree_diff[index] = res[2]
# names[index] = "Glass"
# index = index + 1
# 
# 
# nb_nbtree_diff
# c4.5_nbtree_diff
# nb_nbtree_error
# c4.5_nbtree_error

# Accuravy plot
plot( nb_nbtree_diff, type = "l", ylim=c(-20,40), axes=FALSE, ylab="Accuracy Difference(%)", xlab = "")
par(new=TRUE)
plot(c4.5_nbtree_diff, type = "l", ylim=c(-20,40), axes=FALSE, xlab = "", ylab = "", lty= 6)

axis(2)
axis(1, at=seq_along(nb_nbtree_diff),labels=as.character(names), las=2)
box()
legend("topleft",inset = .01,c("NBTree-C4.5","NBTree-NB"),lty = c(6, 1),x.intersp=0.5, y.intersp=0.8)
grid(lty = 1)


# Error plot
index = 1
final_nb_nbtree_error = c()
final_c4.5_nbtree_error = c()
final_names = c()
for(i in 1:length(c4.5_nbtree_error)){
  final_c4.5_nbtree_error[index] = c4.5_nbtree_error[i];
  final_names[index] = names[i];
  index = index + 1
  final_c4.5_nbtree_error[index] = 0
  final_names[index] = ""
  index = index + 1
}
index = 1

for(i in 1:length(nb_nbtree_error)){
  final_nb_nbtree_error[index] = 0
  index = index + 1
  final_nb_nbtree_error[index] = nb_nbtree_error[i];
  index = index + 1
}
plot( final_c4.5_nbtree_error, type = "h", ylim=c(0,2), axes=FALSE, ylab="Error Ratio", col = "red",  xlab="", lwd = 5)
par(new=TRUE)
plot(final_nb_nbtree_error, type = "h", ylim=c(0,2), axes=FALSE, xlab = "", ylab = "", col = "blue", lwd = 5)

axis(2)
axis(1, at=seq_along(final_nb_nbtree_error),labels=as.character(final_names), las=2)
box()
legend("topleft",inset = .01,c("NBTree/C4.5","NBTree/NB"),lty=c(1, 1), col = c("red","blue"),x.intersp=0.5, y.intersp=0.8)
grid(lty = 1)


# Improvement plot
plot( nb_nbtree_diff, type = "l", ylim=c(-20,40), axes=FALSE, ylab="Accuracy Difference(%)", xlab = "")
par(new=TRUE)
plot(c4.5_nbtree_diff, type = "l", ylim=c(-20,40), axes=FALSE, xlab = "", ylab = "", lty= 6)
par(new=TRUE)
plot(c5_nbtree_diff, type = "l", ylim=c(-20,40), axes=FALSE, xlab = "", ylab = "", lty= 2, lwd = 2)

axis(2)
axis(1, at=seq_along(nb_nbtree_diff),labels=as.character(names), las=2)
box()
legend("topleft",inset = .01,c("NBTree-C4.5","NBTree-NB", "NBTree-C5.0"),lty = c(6, 1, 2),x.intersp=0.5, y.intersp=0.8)
grid(lty = 1)







